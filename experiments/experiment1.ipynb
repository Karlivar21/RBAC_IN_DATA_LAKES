{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaaa5bf4",
   "metadata": {},
   "source": [
    "# Experiment 1 â€“ Show Encrypted Data Cannot Be Read\n",
    "\n",
    "**Goal:** Demonstrate that encrypted data stored in an AWS S3 bucket cannot be read\n",
    "without the appropriate decryption key / permissions.\n",
    "\n",
    "We will:\n",
    "\n",
    "1. List objects in the test S3 bucket.\n",
    "2. Attempt to read an *encrypted* Parquet file from S3 using Python (`boto3` + `pandas`).\n",
    "3. Attempt to read the same encrypted data via Snowflake external stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e43a27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import io\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "from cryptography.fernet import Fernet\n",
    "from io import BytesIO\n",
    "from botocore.exceptions import NoCredentialsError, ClientError\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4d76e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef85bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"rher-s3-test-bucket\"\n",
    "INPUT_FILE = \"sample_sensitive_data.parquet\"\n",
    "OUTPUT_FILE_LOCATION = \"sample_sensitive_data_encrypted.parquet\"\n",
    "COLUMN_TO_ENCRYPT = \"salary\" \n",
    "FERNET_KEY = \"U1eIY6p4bKjOaMycX1VyMshD0tRmfWqC7xJ0MMT8oO0=\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f6fc8",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a45ae307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_s3_objects(bucket_name):\n",
    "    try:\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "        if 'Contents' not in response:\n",
    "            print(f\"No objects found in bucket: {bucket_name}\")\n",
    "            return []\n",
    "        print(f\"Objects in bucket '{bucket_name}':\")\n",
    "        for obj in response['Contents']:\n",
    "            print(f\" - {obj['Key']} (LastModified: {obj['LastModified']}, Size: {obj['Size']} bytes)\")\n",
    "        return [obj['Key'] for obj in response['Contents']]\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not found. Please configure them first.\")\n",
    "        return []\n",
    "    except ClientError as e:\n",
    "        print(f\" AWS Client Error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\" Unexpected error: {e}\")\n",
    "        return []\n",
    "\n",
    "def read_s3_parquet(bucket_name, object_key):\n",
    "    \"\"\"Read a Parquet file from S3 and print a preview.\"\"\"\n",
    "    try:\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "        data = BytesIO(response['Body'].read())\n",
    "        df = pd.read_parquet(data, engine=\"pyarrow\")\n",
    "        print(f\"\\n successfully loaded '{object_key}' into DataFrame.\")\n",
    "        print(\" Data Preview:\")\n",
    "        print(df.head()) \n",
    "        print(f\"DataFrame shape: {df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error reading Parquet: {e}\")\n",
    "\n",
    "def encrypt_column_in_parquet(input_file, output_file_location, column_to_encrypt, fernet_key):\n",
    "    \"\"\"Encrypt a specific column in a Parquet file and upload the result to S3.\"\"\"\n",
    "    fernet = Fernet(fernet_key)\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    buffer = io.BytesIO()\n",
    "    s3.download_fileobj(BUCKET_NAME, input_file, buffer)\n",
    "    buffer.seek(0)\n",
    "    table = pq.read_table(buffer)\n",
    "    df = table.to_pandas()\n",
    "    print(f\"Encrypting column: {column_to_encrypt}\")\n",
    "    if column_to_encrypt in df.columns:\n",
    "        df[column_to_encrypt] = df[column_to_encrypt].astype(str).apply(\n",
    "            lambda x: fernet.encrypt(x.encode()).decode()\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Column '{column_to_encrypt}' not found in Parquet file\")\n",
    "\n",
    "    output_buffer = io.BytesIO()\n",
    "    pq.write_table(pa.Table.from_pandas(df), output_buffer)\n",
    "    output_buffer.seek(0)\n",
    "    print(\"Uploading encrypted Parquet file to S3...\")\n",
    "    s3.upload_fileobj(output_buffer, BUCKET_NAME, output_file_location)\n",
    "\n",
    "    print(\"Done!\")\n",
    "    print(f\"Encrypted file uploaded to s3://{BUCKET_NAME}/{output_file_location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2aa17a",
   "metadata": {},
   "source": [
    "## Encrypt salary column in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "129c6c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypting column: salary\n",
      "Uploading encrypted Parquet file to S3...\n",
      "Done!\n",
      "Encrypted file uploaded to s3://rher-s3-test-bucket/sample_sensitive_data_encrypted.parquet\n"
     ]
    }
   ],
   "source": [
    "encrypt_column_in_parquet(INPUT_FILE, OUTPUT_FILE_LOCATION, COLUMN_TO_ENCRYPT, FERNET_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680c72ea",
   "metadata": {},
   "source": [
    "## List all .parquet fiels in bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0d9d4db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket 'rher-s3-test-bucket':\n",
      " - sample_sensitive_data.parquet (LastModified: 2025-11-03 11:16:22+00:00, Size: 2152 bytes)\n",
      " - sample_sensitive_data_encrypted.parquet (LastModified: 2025-11-17 20:42:42+00:00, Size: 5303 bytes)\n",
      "\n",
      "Parquet objects found:\n",
      " - sample_sensitive_data.parquet\n",
      " - sample_sensitive_data_encrypted.parquet\n",
      "\n",
      "Encrypted candidates:\n",
      " - sample_sensitive_data_encrypted.parquet\n"
     ]
    }
   ],
   "source": [
    "all_keys = list_s3_objects(BUCKET_NAME)\n",
    "\n",
    "all_parquet_files = [k for k in all_keys if k.endswith(\".parquet\")]\n",
    "print(\"\\nParquet objects found:\")\n",
    "for k in all_parquet_files:\n",
    "    print(\" -\", k)\n",
    "\n",
    "encrypted_candidates = [k for k in all_parquet_files if \"encrypted\" in k.lower()]\n",
    "\n",
    "print(\"\\nEncrypted candidates:\")\n",
    "for k in encrypted_candidates:\n",
    "    print(\" -\", k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ed1af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Read unencrypted file ===\n",
      "\n",
      " successfully loaded 'sample_sensitive_data.parquet' into DataFrame.\n",
      " Data Preview:\n",
      "   id     name                email   department  salary\n",
      "0   1    Alice    alice@example.com           HR   55000\n",
      "1   2      Bob      bob@example.com  Engineering   72000\n",
      "2   3  Charlie  charlie@example.com    Marketing   63000\n",
      "3   4    David    david@example.com      Finance   80000\n",
      "4   5      Eva      eva@example.com  Engineering   75000\n",
      "DataFrame shape: (10, 5)\n",
      "\n",
      " successfully loaded 'sample_sensitive_data_encrypted.parquet' into DataFrame.\n",
      " Data Preview:\n",
      "   id     name                email   department  \\\n",
      "0   1    Alice    alice@example.com           HR   \n",
      "1   2      Bob      bob@example.com  Engineering   \n",
      "2   3  Charlie  charlie@example.com    Marketing   \n",
      "3   4    David    david@example.com      Finance   \n",
      "4   5      Eva      eva@example.com  Engineering   \n",
      "\n",
      "                                              salary  \n",
      "0  gAAAAABpG4i9PqxfyVUSN5fShPPsEE_d_AU--SIMTATsA7...  \n",
      "1  gAAAAABpG4i93EP4KYErsubr2EaqHz08fKmKpROq5XkQ90...  \n",
      "2  gAAAAABpG4i9w4Jm5leymKvDlksha3dcjk-1umQX5RHXo4...  \n",
      "3  gAAAAABpG4i9P5zV5_T_aCNrx3yGCoje9lxAT4zlI-w2hv...  \n",
      "4  gAAAAABpG4i92ui1EDcCDhMpBqft8e1K9L44czjZ7aAoDG...  \n",
      "DataFrame shape: (10, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Read unencrypted file ===\")\n",
    "\n",
    "read_s3_parquet(BUCKET_NAME, all_parquet_files[0])\n",
    "read_s3_parquet(BUCKET_NAME, all_parquet_files[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c30f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbac_in_data_lakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
