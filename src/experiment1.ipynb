{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece1577c",
   "metadata": {},
   "source": [
    "# Experiment 1 â€“ Show Encrypted Data Cannot Be Read\n",
    "\n",
    "**Goal:** Demonstrate that data stored in an AWS S3 bucket can be retrieved given credentials to the bucket\n",
    "\n",
    "The experiment:\n",
    "\n",
    "1. Lists objects in the test S3 bucket.\n",
    "2. Retrieves the files and shows the data stored in it (which contains encrypted information)"
   ]
  },
  {
   "cell_type": "code",
   "id": "50802b94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:50:38.118847Z",
     "start_time": "2025-12-11T18:50:38.062991Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from utility_functions import *\n",
    "from constants import *\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "3e3b9226",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:50:40.982329Z",
     "start_time": "2025-12-11T18:50:40.471121Z"
    }
   },
   "source": [
    "s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=os.getenv(\"AWS_ROOT_ACCESS_KEY_ID\"),\n",
    "        aws_secret_access_key=os.getenv(\"AWS_ROOT_SECRET_ACCESS_KEY\"),\n",
    "        region_name=REGION\n",
    ")\n",
    "        \n",
    "list_s3_bucket_objects(s3_client)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objects in bucket 's3-rbac-in-data-lakes-experiments':\n",
      " - employee_data_encrypted.parquet\n",
      " - employee_data_raw.parquet\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "701ab7ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:50:42.764118Z",
     "start_time": "2025-12-11T18:50:42.610930Z"
    }
   },
   "source": [
    "data = get_data(s3_client, EMPLOYEE_DATA_RAW_KEY)\n",
    "pd.read_parquet(data, engine=\"pyarrow\").head(3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " successfully loaded 'employee_data_raw.parquet' from S3 bucket 's3-rbac-in-data-lakes-experiments'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   ID     Name                Email   Department  Salary        Password\n",
       "0   1    Alice    alice@example.com           HR   55000  DummyPassword1\n",
       "1   2      Bob      bob@example.com  Engineering   72000  DummyPassword2\n",
       "2   3  Charlie  charlie@example.com    Marketing   63000  DummyPassword3"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Email</th>\n",
       "      <th>Department</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Password</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alice</td>\n",
       "      <td>alice@example.com</td>\n",
       "      <td>HR</td>\n",
       "      <td>55000</td>\n",
       "      <td>DummyPassword1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>bob@example.com</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>72000</td>\n",
       "      <td>DummyPassword2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>charlie@example.com</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>63000</td>\n",
       "      <td>DummyPassword3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "90a07ab5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T18:50:46.756216Z",
     "start_time": "2025-12-11T18:50:46.363727Z"
    }
   },
   "source": [
    "data = get_data(s3_client, EMPLOYEE_DATA_ENCRYPTED_KEY)\n",
    "pd.read_parquet(data, engine=\"pyarrow\").head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " successfully loaded 'employee_data_encrypted.parquet' from S3 bucket 's3-rbac-in-data-lakes-experiments'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Could not open Parquet input source '<Buffer>': Could not read encrypted metadata, no decryption found in reader's properties",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m data \u001B[38;5;241m=\u001B[39m get_data(s3_client, EMPLOYEE_DATA_ENCRYPTED_KEY)\n\u001B[1;32m----> 2\u001B[0m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_parquet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpyarrow\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mhead()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pandas\\io\\parquet.py:669\u001B[0m, in \u001B[0;36mread_parquet\u001B[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[0m\n\u001B[0;32m    666\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    667\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[1;32m--> 669\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mread(\n\u001B[0;32m    670\u001B[0m     path,\n\u001B[0;32m    671\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m    672\u001B[0m     filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[0;32m    673\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    674\u001B[0m     use_nullable_dtypes\u001B[38;5;241m=\u001B[39muse_nullable_dtypes,\n\u001B[0;32m    675\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    676\u001B[0m     filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m    677\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    678\u001B[0m )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pandas\\io\\parquet.py:265\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[1;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001B[0m\n\u001B[0;32m    258\u001B[0m path_or_handle, handles, filesystem \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[0;32m    259\u001B[0m     path,\n\u001B[0;32m    260\u001B[0m     filesystem,\n\u001B[0;32m    261\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[0;32m    262\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    263\u001B[0m )\n\u001B[0;32m    264\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 265\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[0;32m    266\u001B[0m         path_or_handle,\n\u001B[0;32m    267\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m    268\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[0;32m    269\u001B[0m         filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[0;32m    270\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    271\u001B[0m     )\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m catch_warnings():\n\u001B[0;32m    274\u001B[0m         filterwarnings(\n\u001B[0;32m    275\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    276\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmake_block is deprecated\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    277\u001B[0m             \u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[0;32m    278\u001B[0m         )\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pyarrow\\parquet\\core.py:1844\u001B[0m, in \u001B[0;36mread_table\u001B[1;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001B[0m\n\u001B[0;32m   1832\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mread_table\u001B[39m(source, \u001B[38;5;241m*\u001B[39m, columns\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, use_threads\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   1833\u001B[0m                schema\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, use_pandas_metadata\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, read_dictionary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1834\u001B[0m                binary_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, list_type\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, memory_map\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, buffer_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1840\u001B[0m                page_checksum_verification\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m   1841\u001B[0m                arrow_extensions_enabled\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1843\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1844\u001B[0m         dataset \u001B[38;5;241m=\u001B[39m \u001B[43mParquetDataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1845\u001B[0m \u001B[43m            \u001B[49m\u001B[43msource\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1846\u001B[0m \u001B[43m            \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1847\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilesystem\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilesystem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1848\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpartitioning\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpartitioning\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1849\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1850\u001B[0m \u001B[43m            \u001B[49m\u001B[43mread_dictionary\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mread_dictionary\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1851\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbinary_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbinary_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1852\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlist_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlist_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1853\u001B[0m \u001B[43m            \u001B[49m\u001B[43mbuffer_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbuffer_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1854\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfilters\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfilters\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1855\u001B[0m \u001B[43m            \u001B[49m\u001B[43mignore_prefixes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_prefixes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1856\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpre_buffer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpre_buffer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1857\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcoerce_int96_timestamp_unit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1858\u001B[0m \u001B[43m            \u001B[49m\u001B[43mdecryption_properties\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecryption_properties\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1859\u001B[0m \u001B[43m            \u001B[49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_string_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthrift_container_size_limit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpage_checksum_verification\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1862\u001B[0m \u001B[43m            \u001B[49m\u001B[43marrow_extensions_enabled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marrow_extensions_enabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1863\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1864\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n\u001B[0;32m   1865\u001B[0m         \u001B[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001B[39;00m\n\u001B[0;32m   1866\u001B[0m         \u001B[38;5;66;03m# module is not available\u001B[39;00m\n\u001B[0;32m   1867\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m filters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pyarrow\\parquet\\core.py:1413\u001B[0m, in \u001B[0;36mParquetDataset.__init__\u001B[1;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001B[0m\n\u001B[0;32m   1409\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m single_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1410\u001B[0m     fragment \u001B[38;5;241m=\u001B[39m parquet_format\u001B[38;5;241m.\u001B[39mmake_fragment(single_file, filesystem)\n\u001B[0;32m   1412\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset \u001B[38;5;241m=\u001B[39m ds\u001B[38;5;241m.\u001B[39mFileSystemDataset(\n\u001B[1;32m-> 1413\u001B[0m         [fragment], schema\u001B[38;5;241m=\u001B[39mschema \u001B[38;5;129;01mor\u001B[39;00m \u001B[43mfragment\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mphysical_schema\u001B[49m,\n\u001B[0;32m   1414\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39mparquet_format,\n\u001B[0;32m   1415\u001B[0m         filesystem\u001B[38;5;241m=\u001B[39mfragment\u001B[38;5;241m.\u001B[39mfilesystem\n\u001B[0;32m   1416\u001B[0m     )\n\u001B[0;32m   1417\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m   1419\u001B[0m \u001B[38;5;66;03m# check partitioning to enable dictionary encoding\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pyarrow\\_dataset.pyx:1477\u001B[0m, in \u001B[0;36mpyarrow._dataset.Fragment.physical_schema.__get__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pyarrow\\error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\rbac_in_data_lakes\\lib\\site-packages\\pyarrow\\error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mOSError\u001B[0m: Could not open Parquet input source '<Buffer>': Could not read encrypted metadata, no decryption found in reader's properties"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bada3ed4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rbac_in_data_lakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
